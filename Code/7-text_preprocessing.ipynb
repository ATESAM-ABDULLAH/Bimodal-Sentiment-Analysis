{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDxxDHCcz9uc"
   },
   "source": [
    "# Import Packages + Move to Data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0HTUQPhZxPu",
    "outputId": "4ca68cad-cb5b-4a4f-fcfa-8e8c9573cccc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os  # File handling\n",
    "import pandas as pd  # Reading Csv\n",
    "import numpy as np  # np.pad for padding our embedings\n",
    "import pickle  # For saving data\n",
    "\n",
    "import re  # regex\n",
    "from nltk.tokenize import word_tokenize  # Tokenize the sentence\n",
    "import nltk  # NLP library\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from gensim.models import KeyedVectors  # Read a glove file as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5fdgmEzWc-H",
    "outputId": "296bffa8-945b-4c1f-adbd-21a18897304c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Connect to drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kc4uc8uIZvFt"
   },
   "outputs": [],
   "source": [
    "# Move to data directory\n",
    "os.chdir(\"drive/MyDrive/BSA-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTabFzU_0DK_"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ay1hlrZDaIjN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f4262f5-40df-4675-a60e-c4ca7fa3d2bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Max sentence lengths\n",
      "\n",
      "Test: 236 words\n",
      "Dev: 181 words\n",
      "Train: 327 words\n"
     ]
    }
   ],
   "source": [
    "sep = \",\"\n",
    "\n",
    "print(\"Max sentence lengths\\n\")\n",
    "\n",
    "# Read text data from csv as dataframe\n",
    "test_df = pd.read_csv(\"test.csv\", sep=sep)\n",
    "max_test_len = test_df[\"Utterance\"].str.len().max()  # max length of sentence in data\n",
    "print(f\"Test: {max_test_len} words\")\n",
    "\n",
    "dev_df = pd.read_csv(\"dev.csv\", sep=sep)\n",
    "max_dev_len = dev_df[\"Utterance\"].str.len().max()\n",
    "print(f\"Dev: {max_dev_len} words\")\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\", sep=sep)\n",
    "max_train_len = train_df[\"Utterance\"].str.len().max()\n",
    "print(f\"Train: {max_train_len} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vByitAJqzw-j"
   },
   "source": [
    "## Download Glove Embeddings\n",
    "Run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hqQK-lXOecvL"
   },
   "outputs": [],
   "source": [
    "# # Download Glove embeddings: Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "# !wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
    "\n",
    "# # Unzip file\n",
    "# !unzip glove.840B.300d.zip\n",
    "\n",
    "# # Delete the zip\n",
    "# !rm glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5afXTR_XuNOW"
   },
   "outputs": [],
   "source": [
    "# Choose your GloVe file path\n",
    "glove_model_path = \"glove.840B.300d.txt\"  # Input file\n",
    "\n",
    "# Vocab length -> pad all to this size\n",
    "vocab_length = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "id": "wPmauT7eKBh4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1snme41NfnoP"
   },
   "outputs": [],
   "source": [
    "# Function to take a string and return clean list of words\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove punctuation, stop words, and extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove spaces\n",
    "    tokens = word_tokenize(text)  # Tokenize sentence -> split into small parts\n",
    "    return tokens  # return list\n",
    "\n",
    "\n",
    "# Function to find embeddings and process everything\n",
    "def preprocess_and_embed_data(df, model, column=\"Utterance\", max_length=350):\n",
    "    \"\"\"Preprocesses text data, finds word embeddings, and pads sequences.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the \"Utterances\" column.\n",
    "        model (KeyedVectors): Loaded word embedding model.\n",
    "        max_length (int, optional): Maximum length for padding. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list: List of padded word embedding sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for utterance in df[column]:\n",
    "        # Preprocess text\n",
    "        words = clean_text(utterance)\n",
    "\n",
    "        # Find word embeddings if not return Null filled list\n",
    "        word_embeddings = [model[word] for word in words if word in model] or (\n",
    "            [[0] * 300]\n",
    "        )\n",
    "\n",
    "        # Pad or truncate sequences\n",
    "        padded_embeddings = np.pad(\n",
    "            np.array(word_embeddings),\n",
    "            [(0, max_length - len(word_embeddings)), (0, 0)],\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        processed_data.append(padded_embeddings)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read The GloVe file and save Model\n",
    "Run only once"
   ],
   "metadata": {
    "id": "nc_1mV348FIq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Q-raqCcKuwfh"
   },
   "outputs": [],
   "source": [
    "# # Read the glove file we downloaded -> takes several minutes\n",
    "#  glove_model = KeyedVectors.load_word2vec_format(glove_model_path, binary=False, no_header=True)\n",
    "# print(f\"Loaded GloVe model with {len(glove_model)} words\")\n",
    "\n",
    "# # Save the model for faster usage\n",
    "# glove_model.save(\"glove_model.bin\")\n",
    "\n",
    "# # Remove the glove file\n",
    "# !rm glove.840B.300d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read saved model"
   ],
   "metadata": {
    "id": "GgOwLnjnKm6B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the saved Model file -> takes 1-2 mins max\n",
    "glove_model = KeyedVectors.load(\"glove_model.bin\")\n",
    "print(f\"Loaded GloVe model with {len(glove_model)} words\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXGql_UJ7PKr",
    "outputId": "0d4abba3-4dd7-4f70-d4db-a3d7eb52b7fb"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded GloVe model with 2196018 words\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the Embeddings\n",
    "Hierarchy: sentence =  350 words x 300 embeddings\n",
    "- clean_data = all sentences\n",
    "- clean_data -> sentence -> word -> list[float32]"
   ],
   "metadata": {
    "id": "UNLJ43ckIqck"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clean_data = preprocess_and_embed_data(dev_df, glove_model)\n",
    "dev_df[\"Word_embeddings\"] = clean_data\n",
    "pickle.dump(dev_df, open(\"dev_df_processed.pkl\", \"wb\"))  # save df as pickle file\n",
    "del clean_data\n",
    "\n",
    "clean_data = preprocess_and_embed_data(test_df, glove_model)\n",
    "test_df[\"Word_embeddings\"] = clean_data\n",
    "pickle.dump(test_df, open(\"test_df_processed.pkl\", \"wb\"))  # save df as pickle file\n",
    "del clean_data\n",
    "\n",
    "clean_data = preprocess_and_embed_data(train_df, glove_model)\n",
    "train_df[\"Word_embeddings\"] = clean_data\n",
    "pickle.dump(train_df, open(\"train_df_processed.pkl\", \"wb\"))  # save df as pickle file\n",
    "del clean_data"
   ],
   "metadata": {
    "id": "mamMNzL-HQ2d"
   },
   "execution_count": 10,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}